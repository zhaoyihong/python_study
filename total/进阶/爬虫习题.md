 
 
## 习题1：抓取网易新闻评论 ##

    url :"http://money.163.com/special/pinglun/"
    抓取第一页的新闻信息，并按照以下规格输出。
    [
    
      {
      'title':'生鲜电商为何难盈利？','created_at':'2013-05-03 08:43','url':'http://money.163.com/13/0503/08/8TUHSEEI00254ITK.html'}
      
      {
        'title':'生鲜电商为何难盈利？','created_at':'2013-05-03 08:43','url':'http://money.163.com/13/0503/08/8TUHSEEI00254ITK.ht
	  '}
    ]

使用正则表达式：

	import urllib
	import re
	
	print "正在获取网页..."
	content = urllib.urlopen("http://money.163.com/special/pinglun/").read()
	assert content != None,"无法获取网页内容"
	print "正在解析内容..."
	regex = re.compile('<div class="item_top">.*?<h2><a href=(.*?)>(.*?)</a>.*?<span class="time">(.*?)<.*?<div class="item_bottom">',re.S)
	content_list = re.findall(regex,content)
	print "内容解析完毕"
	
	print "正在输出内容"
	
	result_list = list()
	
	for x in content_list:
	    result_list.append(dict([("title",x[1]),("url",x[0]),("time",x[2])]))
	
	print content_list

 
使用BeautifulSoup4来做这题
 

	
	import urllib
	import bs4
	
	result_list = list()
	print "正在获取网页..."
	content = urllib.urlopen("http://money.163.com/special/pinglun/").read()
	assert content != None,"无法获取网页内容"
	print "正在解析内容..."
	soup = bs4.BeautifulSoup(content)
	for top_tag in soup.find_all('div',class_='item_top'):
	    #标题和链接
	    a_tag = top_tag.find('h2').find('a') #find查找子节点中的第一个 
	    href = a_tag['href'] #获取href属性
	    title = a_tag.string #获取text
	    #time
	    span_tag = top_tag.find('p').find('span')
	    time = span_tag.string
	    result_list.append({'title':title,'time':time,'href':href})
	
	print '正在输出内容'
	print result_list

 
作业2 解析京东商品信息

	url: "http://search.jd.com/Search?keyword=%E5%B9%BC%E7%8C%AB%E7%8C%AB%E7%B2%AE&enc=utf-8#filter"
	
	print jd_search(keyword)
	
	[dict,dict,dict]
	
	dict {pic:'',title:'',price:'',url:''}

	 
	import sys
	import urllib
	import bs4
	
	def jd_search(keyword,page_start,page_limit):
	    '''
	    抓取并解析京东的页面，并解析出内容. 
	    @keyword 关键词
	    @page_start 开始页面
	    @page_limit 抓取的页面数
	    打印出所有商品的标题，价格，链接，图片
	    '''
	    if page_start <= 0:
	        print "起始页数小于0,退出"
	        return 
	    
	    #url模板
	    url_model = 'http://search.jd.com/Search?keyword={KEY_WORD}&enc=utf-8&page={PAGE}'
	    #测试是否有用户输入的页数,先打开第一页，然后查看有几页
	    url_first = url_model.format(KEY_WORD=keyword,PAGE=1)
	    try:
	        file_obj= urllib.urlopen(url_first)
	    except IOError:
	        print "url无法打开",url_first
	        sys.exit(-1)
	    except:
	        print "urlopen 错误"
	        sys.exit(-1)
	    
	    content = file_obj.read()
	    assert content != None,'页面内容读取失败'
	    
	    soup = bs4.BeautifulSoup(content)
	    total_page_str = soup.find(class_='page-skip').find('em').string
	    regex_page = re.compile(u'共(\d+?)页')
	    total_page = int(re.search(regex_page,total_page_str).groups()[0])
	    
	    page_end = page_start+page_limit
	
	    if page_start > total_page:
	        print "起始值超过总页数"
	        return 
	
	    if page_end > total_page:
	        print "结束页超过总页数，自动修正为总页数"
	        page_end = total_page 
	
	    print "page:",page_start,page_end
	
	    for page_num in xrange(page_start,page_end+1):
	        url = url_model.format(KEY_WORD=keyword,PAGE=page_num)
	        if page_num != 1:
	            #抓取页面
	            try:
	                content = urllib.urlopen(url).read()
	            except IOError:
	                print "url打开失败",url
	            except:
	                print "urlopen error"
	            
	            soup = bs4.BeautifulSoup(content)
	        else:
	            #如果page_num == 1,刚才已经抓过了，不用再抓了
	            pass  
	        
	    
	        assert None != content,"网页读取为空"
	       
	
	        result_list = list()
	
	        list_tag = soup.find("ul",class_='list-h clearfix')
	        if list_tag == None:
	            print url+"解析失败"
	            continue
	        for child in list_tag.children:
	            if child.name !=  None:
	                #图片
	                img_tag = child.find("div",class_='p-img')
	                if img_tag != None:
	                    img_src = img_tag.a.img['data-lazyload']
	                #标题 , 链接
	                title_tag = child.find("div",class_='p-name')
	                if title_tag != None:
	                    title_text = title_tag.a.get_text(strip=True)
	                    url = title_tag.a['href']
	                #价格
	                price_tag = child.find('div',class_='p-price')
	                if price_tag != None:
	                    price_text = price_tag.get_text().strip()
	                    price_float = float(price_text.replace(u'￥',''))
	                #print img_src,title_text,url,price_float
	                result_list.append({'keyword':keyword,'page':page_num,'img':img_src,\
	                                    'title':title_text,'href':href,'price':price_float})
	
	        for x in result_list:
	            for k,v in x.items():
	                print "%s:%s" % (k,v)
	            print
	jd_search('幼年猫粮',1,10)
	jd_search('女包',5,2)
	
